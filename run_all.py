"""
å®Œæ•´çš„ç«¯åˆ°ç«¯è¿è¡Œè„šæœ¬
æŒ‰é¡ºåºæ‰§è¡Œæ‰€æœ‰æ­¥éª¤
"""

import os
import sys
import yaml
import json
from datetime import datetime

def print_step(step_num, title):
    """æ‰“å°æ­¥éª¤æ ‡é¢˜"""
    print("\n" + "="*60)
    print(f"æ­¥éª¤ {step_num}: {title}")
    print("="*60 + "\n")

def main():
    print("ğŸš€ MiniELM å®Œæ•´è®­ç»ƒæµç¨‹å¼€å§‹")
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # åŠ è½½é…ç½®
    with open("config/config.yaml", 'r') as f:
        config = yaml.safe_load(f)
    
    # ========== æ­¥éª¤1: ä¸‹è½½æ•°æ® ==========
    print_step(1, "ä¸‹è½½ESCIæ•°æ®é›†")
    from data.download_esci import download_esci_dataset, preprocess_esci
    
    dataset = download_esci_dataset()
    relevant_data = preprocess_esci(dataset)
    
    # ========== æ­¥éª¤2: æ„å»ºQ2Q ==========
    print_step(2, "æ„å»ºQ2Qæ•°æ®é›†")
    from data.build_q2q import build_q2q_dataset
    
    q2q_data = build_q2q_dataset(
        relevant_data,
        min_common_products=config['data']['q2q_min_common_products'],
        max_pairs=5000,
        use_llm_filter=True  # ä¼šè°ƒç”¨API
    )
    
    # ========== æ­¥éª¤3: è®¾ç½®ES ==========
    print_step(3, "è®¾ç½®Elasticsearch")
    from data.setup_elasticsearch import setup_elasticsearch
    
    es = setup_elasticsearch(dataset)
    
    if es is None:
        print("âŒ Elasticsearchæœªå¯åŠ¨ï¼Œè¯·å…ˆå¯åŠ¨ES")
        sys.exit(1)
    
    # ========== æ­¥éª¤4: è®­ç»ƒç›¸å…³æ€§æ¨¡å‹ ==========
    print_step(4, "è®­ç»ƒç›¸å…³æ€§è¯„åˆ†æ¨¡å‹")
    from models.relevance_scorer import RelevanceScorer
    
    relevance_scorer = RelevanceScorer(
        model_name=config['models']['relevance_scorer']['name']
    )
    train_data, val_data = relevance_scorer.prepare_dataset(dataset)
    relevance_scorer.train(train_data, val_data)
    
    # ========== æ­¥éª¤5: SFTè®­ç»ƒStudent ==========
    print_step(5, "SFTè®­ç»ƒStudentæ¨¡å‹")
    from training.sft_trainer import SFTTrainer
    
    student_trainer = SFTTrainer(
        model_name=config['models']['student']['name'],
        is_teacher=False
    )
    train_q2q, val_q2q = student_trainer.load_q2q_data()
    student_trainer.train(
        train_q2q,
        val_q2q,
        num_epochs=config['training']['sft']['num_epochs'],
        batch_size=config['training']['sft']['batch_size'],
        learning_rate=config['training']['sft']['learning_rate']
    )
    
    # ========== æ­¥éª¤6: çŸ¥è¯†è’¸é¦ ==========
    print_step(6, "çŸ¥è¯†è’¸é¦ï¼ˆTeacher â†’ Studentï¼‰")
    from training.distillation import DistillationTrainer
    from torch.utils.data import DataLoader
    
    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å‡†å¤‡å¥½dataloader
    # ç®€åŒ–ç‰ˆï¼šç›´æ¥åŠ è½½å·²æœ‰çš„Studentä½œä¸ºMiniELM
    print("âš  è·³è¿‡è’¸é¦æ­¥éª¤ï¼ˆå¯é€‰ä¼˜åŒ–ï¼‰")
    print("å¦‚éœ€å®Œæ•´è’¸é¦ï¼Œè¯·æ‰‹åŠ¨è¿è¡Œtraining/distillation.py")
    
    # ========== æ­¥éª¤7: åœ¨çº¿DPO ==========
    print_step(7, "åœ¨çº¿DPOè®­ç»ƒ")
    from training.online_dpo import OnlineDPOTrainer, generate_user_profiles
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    train_queries = [item['query'] for item in q2q_data[:1000]]  # å–å‰1000ä¸ªquery
    user_profiles = generate_user_profiles(100)
    
    dpo_trainer = OnlineDPOTrainer(
        model_path="./checkpoints/student_sft",  # ä½¿ç”¨SFTåçš„æ¨¡å‹
        config=config,
        use_llm_feedback=True  # ä¼šè°ƒç”¨API
    )
    
    dpo_trainer.train(
        queries=train_queries,
        user_profiles=user_profiles,
        iterations=config['training']['dpo']['iterations'],
        learning_rate=config['training']['dpo']['learning_rate']
    )
    
    # ========== æ­¥éª¤8: è¯„ä¼° ==========
    print_step(8, "æ¨¡å‹è¯„ä¼°")
    from evaluation.evaluator import Evaluator
    from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
    
    # åŠ è½½æœ€ç»ˆæ¨¡å‹
    final_model = AutoModelForSeq2SeqLM.from_pretrained("./checkpoints/minielm_dpo")
    final_tokenizer = AutoTokenizer.from_pretrained("./checkpoints/minielm_dpo")
    
    evaluator = Evaluator(
        model=final_model,
        tokenizer=final_tokenizer,
        config=config,
        use_llm_feedback=True
    )
    
    # ç¦»çº¿è¯„ä¼°
    test_q2q = val_q2q.select(range(100))  # å–100ä¸ªæ ·æœ¬
    offline_results = evaluator.evaluate_offline(test_q2q)
    
    # åœ¨çº¿è¯„ä¼°
    test_queries = list(set([item['query'] for item in q2q_data[-500:]]))[:100]
    online_results = evaluator.evaluate_online(test_queries, num_rewrites=10)
    
    # ä¿å­˜ç»“æœ
    all_results = {
        'offline': offline_results,
        'online': online_results,
        'timestamp': datetime.now().isoformat()
    }
    
    output_dir = config['system']['output_dir']
    os.makedirs(output_dir, exist_ok=True)
    
    evaluator.save_results(all_results, f"{output_dir}/final_results.json")
    
    # ========== å®Œæˆ ==========
    print("\n" + "="*60)
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print("="*60)
    print(f"\nç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"\nç»“æœæ‘˜è¦:")
    print(f"  ç¦»çº¿ - ExactMatch: {offline_results['exact_match']:.4f}")
    print(f"  ç¦»çº¿ - RougeL: {offline_results['rouge_l']:.4f}")
    print(f"  åœ¨çº¿ - Product Coverage: {online_results['product_coverage']}")
    print(f"  åœ¨çº¿ - Relevance: {online_results['relevance']:.4f}")
    print(f"  åœ¨çº¿ - Diversity: {online_results['diversity']:.4f}")
    
    if 'click_rate' in online_results:
        print(f"  åœ¨çº¿ - Click Rate: {online_results['click_rate']:.4f}")
        print(f"  åœ¨çº¿ - Add-to-Cart Rate: {online_results['add_to_cart_rate']:.4f}")
        print(f"  åœ¨çº¿ - Purchase Rate: {online_results['purchase_rate']:.4f}")
    
    print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_dir}/final_results.json")
    print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜åˆ°: ./checkpoints/minielm_dpo")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()