"""
ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è®­ç»ƒå™¨
ç”¨äºåœ¨Q2Qæ•°æ®é›†ä¸Šè®­ç»ƒTeacherå’ŒStudent
"""

import torch
from transformers import (
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    DataCollatorForSeq2Seq
)
from datasets import Dataset
import json

class SFTTrainer:
    def __init__(self, model_name: str, is_teacher: bool = False):
        """
        åˆå§‹åŒ–SFTè®­ç»ƒå™¨
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼ˆå¦‚google/flan-t5-baseï¼‰
            is_teacher: æ˜¯å¦æ˜¯Teacheræ¨¡å‹
        """
        self.model_name = model_name
        self.is_teacher = is_teacher
        
        print(f"\n{'ğŸ“ Teacher' if is_teacher else 'ğŸ’ Student'} æ¨¡å‹åˆå§‹åŒ–: {model_name}")
        
        # åŠ è½½æ¨¡å‹å’Œtokenizer
        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # ç»Ÿè®¡å‚æ•°é‡
        num_params = sum(p.numel() for p in self.model.parameters())
        print(f"   å‚æ•°é‡: {num_params/1e6:.1f}M")
    
    def load_q2q_data(self, data_path: str = "./data/q2q_dataset.json"):
        """åŠ è½½Q2Qæ•°æ®é›†"""
        with open(data_path, 'r') as f:
            q2q_data = json.load(f)
        
        print(f"âœ“ åŠ è½½Q2Qæ•°æ®: {len(q2q_data)} æ¡")
        
        # è½¬ä¸ºDataset
        dataset = Dataset.from_list(q2q_data)
        
        # åˆ’åˆ†train/val
        split = dataset.train_test_split(test_size=0.1, seed=42)
        
        return split['train'], split['test']
    
    def preprocess_function(self, examples):
        """é¢„å¤„ç†å‡½æ•°ï¼šå°†queryè½¬ä¸ºinput_idsï¼Œrewritten_queryè½¬ä¸ºlabels"""
        
        # è¾“å…¥ï¼šåŸå§‹query
        # æ·»åŠ ä»»åŠ¡å‰ç¼€ï¼ˆT5é£æ ¼ï¼‰
        inputs = [f"rewrite query: {q}" for q in examples['query']]
        
        # ç›®æ ‡ï¼šæ”¹å†™åçš„query
        targets = examples['rewritten_query']
        
        # Tokenize
        model_inputs = self.tokenizer(
            inputs,
            max_length=128,
            truncation=True,
            padding='max_length'
        )
        
        # Tokenize targets
        labels = self.tokenizer(
            targets,
            max_length=128,
            truncation=True,
            padding='max_length'
        )
        
        model_inputs['labels'] = labels['input_ids']
        
        return model_inputs
    
    def train(self, 
              train_dataset,
              val_dataset,
              output_dir: str = None,
              num_epochs: int = 3,
              batch_size: int = 8,
              learning_rate: float = 5e-5):
        """
        è®­ç»ƒæ¨¡å‹
        """
        if output_dir is None:
            output_dir = f"./checkpoints/{'teacher' if self.is_teacher else 'student'}_sft"
        
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ...")
        print(f"   è¾“å‡ºç›®å½•: {output_dir}")
        
        # é¢„å¤„ç†æ•°æ®
        train_dataset = train_dataset.map(
            self.preprocess_function,
            batched=True,
            remove_columns=train_dataset.column_names
        )
        
        val_dataset = val_dataset.map(
            self.preprocess_function,
            batched=True,
            remove_columns=val_dataset.column_names
        )
        
        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=num_epochs,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size * 2,
            gradient_accumulation_steps=4,  # æœ‰æ•ˆbatch_size = 8*4=32
            learning_rate=learning_rate,
            warmup_steps=500,
            weight_decay=0.01,
            logging_steps=50,
            evaluation_strategy="steps",
            eval_steps=200,
            save_steps=500,
            save_total_limit=2,
            fp16=True,  # æ··åˆç²¾åº¦
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False
        )
        
        # Data collator
        data_collator = DataCollatorForSeq2Seq(
            self.tokenizer,
            model=self.model,
            padding=True
        )
        
        # Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            data_collator=data_collator
        )
        
        # å¼€å§‹è®­ç»ƒ
        trainer.train()
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.model.save_pretrained(output_dir)
        self.tokenizer.save_pretrained(output_dir)
        
        print(f"âœ“ è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")
        
        return trainer
    
    def generate(self, query: str, num_return_sequences: int = 1, **kwargs) -> list:
        """
        ç”Ÿæˆæ”¹å†™query
        """
        input_text = f"rewrite query: {query}"
        inputs = self.tokenizer(input_text, return_tensors='pt').to(self.model.device)
        
        outputs = self.model.generate(
            **inputs,
            max_length=128,
            num_return_sequences=num_return_sequences,
            do_sample=True,
            temperature=0.8,
            top_p=0.9,
            **kwargs
        )
        
        rewrites = [
            self.tokenizer.decode(output, skip_special_tokens=True)
            for output in outputs
        ]
        
        return rewrites


# ============ æµ‹è¯•ä»£ç  ============
if __name__ == "__main__":
    import yaml
    
    # åŠ è½½é…ç½®
    with open("config/config.yaml", 'r') as f:
        config = yaml.safe_load(f)
    
    # è®­ç»ƒStudentæ¨¡å‹
    trainer = SFTTrainer(
        model_name=config['models']['student']['name'],
        is_teacher=False
    )
    
    # åŠ è½½æ•°æ®
    train_data, val_data = trainer.load_q2q_data()
    
    # è®­ç»ƒ
    trainer.train(train_data, val_data)
    
    # æµ‹è¯•ç”Ÿæˆ
    test_query = "summer dress"
    rewrites = trainer.generate(test_query, num_return_sequences=3)
    print(f"\næµ‹è¯•ç”Ÿæˆ:\nåŸå§‹: {test_query}")
    for i, r in enumerate(rewrites):
        print(f"æ”¹å†™{i+1}: {r}")