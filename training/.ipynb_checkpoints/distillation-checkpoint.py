"""
çŸ¥è¯†è’¸é¦è®­ç»ƒå™¨
ä½¿ç”¨åå‘KLæ•£åº¦å°†TeacherçŸ¥è¯†è¿ç§»åˆ°Student
"""

import torch
import torch.nn.functional as F
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from torch.utils.data import DataLoader
from tqdm import tqdm
import os

class DistillationTrainer:
    def __init__(self, 
                 teacher_path: str,
                 student_path: str,
                 temperature: float = 2.0):
        """
        åˆå§‹åŒ–è’¸é¦è®­ç»ƒå™¨
        
        Args:
            teacher_path: Teacheræ¨¡å‹è·¯å¾„
            student_path: Studentæ¨¡å‹è·¯å¾„ï¼ˆå·²SFTè®­ç»ƒï¼‰
            temperature: è’¸é¦æ¸©åº¦
        """
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.temperature = temperature
        
        print("\nğŸ”¬ åˆå§‹åŒ–çŸ¥è¯†è’¸é¦...")
        
        # åŠ è½½Teacherï¼ˆevalæ¨¡å¼ï¼Œä¸è®­ç»ƒï¼‰
        print(f"   åŠ è½½Teacher: {teacher_path}")
        self.teacher = AutoModelForSeq2SeqLM.from_pretrained(teacher_path).to(self.device)
        self.teacher.eval()
        for param in self.teacher.parameters():
            param.requires_grad = False
        
        # åŠ è½½Studentï¼ˆè®­ç»ƒæ¨¡å¼ï¼‰
        print(f"   åŠ è½½Student: {student_path}")
        self.student = AutoModelForSeq2SeqLM.from_pretrained(student_path).to(self.device)
        self.student.train()
        
        # Tokenizerï¼ˆå…±ç”¨ï¼‰
        self.tokenizer = AutoTokenizer.from_pretrained(teacher_path)
        
        print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def reverse_kl_loss(self, student_logits, teacher_logits):
        """
        åå‘KLæ•£åº¦æŸå¤±ï¼ˆè®ºæ–‡å…¬å¼1ï¼‰
        
        D_KL(P_S || P_T) = Î£ P_S(x) log(P_S(x) / P_T(x))
        
        è®©Studentä¸“æ³¨äºTeacherçš„é«˜æ¦‚ç‡åŒºåŸŸ
        """
        # åº”ç”¨æ¸©åº¦softmax
        student_probs = F.softmax(student_logits / self.temperature, dim=-1)
        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=-1)
        
        # åå‘KLæ•£åº¦
        kl_loss = F.kl_div(
            student_probs.log(),
            teacher_probs,
            reduction='batchmean',
            log_target=False
        )
        
        # æ¸©åº¦æ ¡æ­£
        return kl_loss * (self.temperature ** 2)
    
    def train(self,
              train_dataloader,
              val_dataloader,
              num_epochs: int = 3,
              learning_rate: float = 3e-5,
              output_dir: str = "./checkpoints/minielm"):
        """
        æ‰§è¡Œè’¸é¦è®­ç»ƒ
        """
        print(f"\nğŸ¯ å¼€å§‹çŸ¥è¯†è’¸é¦è®­ç»ƒ...")
        print(f"   Epochs: {num_epochs}")
        print(f"   Learning Rate: {learning_rate}")
        print(f"   Temperature: {self.temperature}")
        
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¼˜åŒ–å™¨
        optimizer = torch.optim.AdamW(
            self.student.parameters(),
            lr=learning_rate
        )
        
        # æ··åˆç²¾åº¦
        scaler = torch.cuda.amp.GradScaler()
        
        best_val_loss = float('inf')
        
        for epoch in range(num_epochs):
            print(f"\nğŸ“– Epoch {epoch+1}/{num_epochs}")
            
            # ========== è®­ç»ƒé˜¶æ®µ ==========
            self.student.train()
            train_loss = 0
            
            pbar = tqdm(train_dataloader, desc="Training")
            for batch_idx, batch in enumerate(pbar):
                # å°†batchç§»åˆ°GPU
                batch = {k: v.to(self.device) for k, v in batch.items()}
                
                # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
                with torch.cuda.amp.autocast():
                    # Teacherç”Ÿæˆsoft labelsï¼ˆä¸è®¡ç®—æ¢¯åº¦ï¼‰
                    with torch.no_grad():
                        teacher_outputs = self.teacher(
                            input_ids=batch['input_ids'],
                            attention_mask=batch['attention_mask'],
                            labels=batch['labels']
                        )
                        teacher_logits = teacher_outputs.logits
                    
                    # Studentå‰å‘ä¼ æ’­
                    student_outputs = self.student(
                        input_ids=batch['input_ids'],
                        attention_mask=batch['attention_mask'],
                        labels=batch['labels']
                    )
                    student_logits = student_outputs.logits
                    
                    # è®¡ç®—åå‘KLæŸå¤±
                    loss = self.reverse_kl_loss(student_logits, teacher_logits)
                
                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
                
                train_loss += loss.item()
                pbar.set_postfix({'loss': f"{loss.item():.4f}"})
                
                # å®šæœŸä¿å­˜checkpoint
                if (batch_idx + 1) % 500 == 0:
                    self.save_checkpoint(output_dir, epoch, batch_idx)
            
            avg_train_loss = train_loss / len(train_dataloader)
            print(f"   è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")
            
            # ========== éªŒè¯é˜¶æ®µ ==========
            self.student.eval()
            val_loss = 0
            
            with torch.no_grad():
                pbar = tqdm(val_dataloader, desc="Validation")
                for batch in pbar:
                    batch = {k: v.to(self.device) for k, v in batch.items()}
                    
                    # Teacher logits
                    teacher_outputs = self.teacher(
                        input_ids=batch['input_ids'],
                        attention_mask=batch['attention_mask'],
                        labels=batch['labels']
                    )
                    
                    # Student logits
                    student_outputs = self.student(
                        input_ids=batch['input_ids'],
                        attention_mask=batch['attention_mask'],
                        labels=batch['labels']
                    )
                    
                    loss = self.reverse_kl_loss(
                        student_outputs.logits,
                        teacher_outputs.logits
                    )
                    
                    val_loss += loss.item()
                    pbar.set_postfix({'loss': f"{loss.item():.4f}"})
            
            avg_val_loss = val_loss / len(val_dataloader)
            print(f"   éªŒè¯æŸå¤±: {avg_val_loss:.4f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                print(f"   ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹...")
                self.student.save_pretrained(output_dir)
                self.tokenizer.save_pretrained(output_dir)
        
        print(f"\nâœ“ è’¸é¦å®Œæˆï¼MiniELMå·²ä¿å­˜åˆ°: {output_dir}")
        return self.student
    
    def save_checkpoint(self, output_dir, epoch, step):
        """ä¿å­˜checkpoint"""
        checkpoint_dir = f"{output_dir}/checkpoint-epoch{epoch}-step{step}"
        self.student.save_pretrained(checkpoint_dir)
        print(f"   ğŸ’¾ Checkpointä¿å­˜: {checkpoint_dir}")


# ============ æµ‹è¯•ä»£ç  ============
if __name__ == "__main__":
    from torch.utils.data import DataLoader
    from training.sft_trainer import SFTTrainer
    import json
    
    # 1. åŠ è½½Q2Qæ•°æ®
    with open("./data/q2q_dataset.json", 'r') as f:
        q2q_data = json.load(f)
    
    # 2. å‡†å¤‡dataloaderï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…åº”è¯¥ç”¨ä¸Šé¢çš„preprocessï¼‰
    # è¿™é‡Œå‡è®¾ä½ å·²ç»æœ‰äº†tokenizedçš„æ•°æ®
    
    # 3. åˆå§‹åŒ–è’¸é¦å™¨
    distiller = DistillationTrainer(
        teacher_path="./checkpoints/teacher_sft",
        student_path="./checkpoints/student_sft",
        temperature=2.0
    )
    
    # 4. è®­ç»ƒï¼ˆéœ€è¦å‡†å¤‡å¥½dataloaderï¼‰
    # distiller.train(train_dataloader, val_dataloader)
    
    print("è¯·å‚è€ƒå®Œæ•´çš„run_pipeline.pyè„šæœ¬è¿è¡Œè’¸é¦")