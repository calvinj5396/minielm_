"""
åœ¨çº¿DPOè®­ç»ƒï¼ˆDirect Preference Optimizationï¼‰
ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æŸ¥è¯¢æ”¹å†™è´¨é‡
"""

import torch
import torch.nn.functional as F
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
import random
from tqdm import tqdm
import os
from utils.llm_api import LLMClient
from utils.elasticsearch_utils import ElasticsearchClient
from models.relevance_scorer import RelevanceScorer

class OnlineDPOTrainer:
    def __init__(self,
                 model_path: str,
                 config: dict,
                 use_llm_feedback: bool = True):
        """
        åˆå§‹åŒ–åœ¨çº¿DPOè®­ç»ƒå™¨
        
        Args:
            model_path: MiniELMæ¨¡å‹è·¯å¾„ï¼ˆè’¸é¦åï¼‰
            config: é…ç½®å­—å…¸
            use_llm_feedback: æ˜¯å¦ä½¿ç”¨LLMæ¨¡æ‹Ÿç”¨æˆ·åé¦ˆ
        """
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.config = config
        self.use_llm_feedback = use_llm_feedback
        
        print("\nğŸ® åˆå§‹åŒ–åœ¨çº¿DPOè®­ç»ƒ...")
        
        # åŠ è½½MiniELM
        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(self.device)
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model.train()
        
        # åŠ è½½è¾…åŠ©æ¨¡å—
        self.relevance_scorer = RelevanceScorer.load_pretrained(
            "./checkpoints/relevance_scorer"
        )
        self.es_client = ElasticsearchClient()
        
        if use_llm_feedback:
            self.llm_client = LLMClient()
            print("   âœ“ LLMåé¦ˆå·²å¯ç”¨")
        else:
            self.llm_client = None
            print("   âš  LLMåé¦ˆå·²ç¦ç”¨ï¼ˆä»…ä½¿ç”¨ç›¸å…³æ€§+å¤šæ ·æ€§ï¼‰")
        
        # DPOå‚æ•°
        self.beta = config['training']['dpo']['beta']
        
        print("âœ“ åˆå§‹åŒ–å®Œæˆ")
    
    def compute_reward(self, 
                       query: str,
                       rewritten_query: str,
                       user_profile: dict = None) -> float:
        """
        è®¡ç®—å¥–åŠ±åˆ†æ•°
        
        ç»„åˆä¸‰ä¸ªæŒ‡æ ‡ï¼š
        1. ç›¸å…³æ€§ (Relevance)
        2. å¤šæ ·æ€§ (Diversity)
        3. ç”¨æˆ·åé¦ˆ (User Feedback) - å¯é€‰
        """
        # 1. æœç´¢å•†å“
        original_products = self.es_client.search(query)
        rewritten_products = self.es_client.search(rewritten_query)
        
        if len(rewritten_products) == 0:
            return 0.0  # æ²¡æœ‰æ£€ç´¢åˆ°å•†å“ï¼Œå¥–åŠ±ä¸º0
        
        # 2. ç›¸å…³æ€§å¾—åˆ†
        relevance_scores = self.relevance_scorer.batch_score(
            query,
            rewritten_products
        )
        relevance = sum(relevance_scores) / len(relevance_scores)
        
        # 3. å¤šæ ·æ€§å¾—åˆ†
        original_ids = set([p['product_id'] for p in original_products])
        rewritten_ids = set([p['product_id'] for p in rewritten_products])
        new_products = len(rewritten_ids - original_ids)
        diversity = new_products / len(original_ids) if len(original_ids) > 0 else 0
        
        # 4. ç”¨æˆ·åé¦ˆå¾—åˆ†ï¼ˆå¯é€‰ï¼‰
        if self.use_llm_feedback and user_profile:
            feedback = self.llm_client.simulate_user_feedback(
                query,
                rewritten_products,
                user_profile
            )
            user_score = (
                0.4 * feedback['click_rate'] +
                0.3 * feedback['add_to_cart_rate'] +
                0.3 * feedback['purchase_rate']
            )
        else:
            user_score = 0
        
        # 5. ç»¼åˆå¥–åŠ±ï¼ˆæƒé‡å¯è°ƒï¼‰
        if self.use_llm_feedback:
            reward = (
                0.4 * relevance +
                0.3 * diversity +
                0.3 * user_score
            )
        else:
            reward = (
                0.6 * relevance +
                0.4 * diversity
            )
        
        return reward
    
    def dpo_loss(self, 
                 query_ids,
                 preferred_ids,
                 rejected_ids):
        """
        DPOæŸå¤±å‡½æ•°ï¼ˆè®ºæ–‡å…¬å¼2ï¼‰
        
        L_DPO = -1/B Î£ log Ïƒ(Î² log(Ï€_Î¸(Q~+|Q) / Ï€_Î¸(Q~-|Q)))
        """
        # è®¡ç®—preferredçš„logæ¦‚ç‡
        preferred_outputs = self.model(
            input_ids=query_ids,
            labels=preferred_ids
        )
        preferred_logprob = -preferred_outputs.loss
        
        # è®¡ç®—rejectedçš„logæ¦‚ç‡
        rejected_outputs = self.model(
            input_ids=query_ids,
            labels=rejected_ids
        )
        rejected_logprob = -rejected_outputs.loss
        
        # DPOæŸå¤±
        logits_diff = self.beta * (preferred_logprob - rejected_logprob)
        loss = -F.logsigmoid(logits_diff).mean()
        
        return loss
    
    def train(self,
              queries: list,
              user_profiles: list,
              iterations: int = 500,
              learning_rate: float = 1e-5,
              output_dir: str = "./checkpoints/minielm_dpo"):
        """
        åœ¨çº¿DPOè®­ç»ƒ
        
        Args:
            queries: è®­ç»ƒæŸ¥è¯¢åˆ—è¡¨
            user_profiles: ç”¨æˆ·ç”»åƒåˆ—è¡¨
            iterations: è®­ç»ƒè¿­ä»£æ¬¡æ•°
            learning_rate: å­¦ä¹ ç‡
            output_dir: è¾“å‡ºç›®å½•
        """
        print(f"\nğŸš€ å¼€å§‹åœ¨çº¿DPOè®­ç»ƒ...")
        print(f"   è¿­ä»£æ¬¡æ•°: {iterations}")
        print(f"   æŸ¥è¯¢æ•°é‡: {len(queries)}")
        print(f"   LLMè¯„ä¼°é¢‘ç‡: æ¯{self.config['training']['dpo']['llm_eval_frequency']}æ­¥")
        
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¼˜åŒ–å™¨
        optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=learning_rate
        )
        
        # è®­ç»ƒå¾ªç¯
        pbar = tqdm(range(iterations), desc="DPO Training")
        total_loss = 0
        
        for iteration in pbar:
            # 1. éšæœºé‡‡æ ·queryå’Œuser profile
            query = random.choice(queries)
            user_profile = random.choice(user_profiles)
            
            # 2. ç”Ÿæˆä¸¤ä¸ªå€™é€‰æ”¹å†™
            rewrite_1 = self.generate_rewrite(query, temperature=0.7)
            rewrite_2 = self.generate_rewrite(query, temperature=0.9)
            
            # 3. è®¡ç®—å¥–åŠ±ï¼ˆæ¯Næ­¥æ‰ç”¨LLMï¼‰
            use_llm = (iteration % self.config['training']['dpo']['llm_eval_frequency'] == 0)
            
            if use_llm and self.use_llm_feedback:
                reward_1 = self.compute_reward(query, rewrite_1, user_profile)
                reward_2 = self.compute_reward(query, rewrite_2, user_profile)
            else:
                # åªç”¨ç›¸å…³æ€§+å¤šæ ·æ€§ï¼ˆå¿«é€Ÿï¼‰
                reward_1 = self.compute_reward(query, rewrite_1, user_profile=None)
                reward_2 = self.compute_reward(query, rewrite_2, user_profile=None)
            
            # 4. ç¡®å®špreferredå’Œrejected
            if reward_1 > reward_2:
                preferred, rejected = rewrite_1, rewrite_2
            else:
                preferred, rejected = rewrite_2, rewrite_1
            
            # 5. Tokenize
            query_input = f"rewrite query: {query}"
            query_ids = self.tokenizer(
                query_input,
                return_tensors='pt',
                max_length=128,
                truncation=True
            ).input_ids.to(self.device)
            
            preferred_ids = self.tokenizer(
                preferred,
                return_tensors='pt',
                max_length=128,
                truncation=True
            ).input_ids.to(self.device)
            
            rejected_ids = self.tokenizer(
                rejected,
                return_tensors='pt',
                max_length=128,
                truncation=True
            ).input_ids.to(self.device)
            
            # 6. è®¡ç®—DPOæŸå¤±å¹¶æ›´æ–°
            loss = self.dpo_loss(query_ids, preferred_ids, rejected_ids)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # 7. è®°å½•
            total_loss += loss.item()
            pbar.set_postfix({
                'loss': f"{loss.item():.4f}",
                'avg_loss': f"{total_loss/(iteration+1):.4f}"
            })
            
            # 8. å®šæœŸä¿å­˜
            if (iteration + 1) % 50 == 0:
                checkpoint_dir = f"{output_dir}/checkpoint-{iteration+1}"
                self.model.save_pretrained(checkpoint_dir)
                print(f"\n   ğŸ’¾ Checkpointä¿å­˜: {checkpoint_dir}")
        
        # æœ€ç»ˆä¿å­˜
        self.model.save_pretrained(output_dir)
        self.tokenizer.save_pretrained(output_dir)
        
        print(f"\nâœ“ DPOè®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")
        
        return self.model
    
    def generate_rewrite(self, query: str, temperature: float = 0.8) -> str:
        """ç”Ÿæˆå•ä¸ªæ”¹å†™"""
        input_text = f"rewrite query: {query}"
        inputs = self.tokenizer(input_text, return_tensors='pt').to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=128,
                do_sample=True,
                temperature=temperature,
                top_p=0.9
            )
        
        rewrite = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return rewrite


# ============ ç”¨æˆ·ç”»åƒç”Ÿæˆ ============
def generate_user_profiles(num_profiles: int = 100) -> list:
    """
    ç”Ÿæˆæ¨¡æ‹Ÿç”¨æˆ·ç”»åƒæ± 
    """
    profiles = []
    
    genders = ['Male', 'Female', 'Other']
    ages = ['18-25', '26-35', '36-50', '50+']
    locations = ['North America', 'Europe', 'Asia']
    incomes = ['Low', 'Middle', 'High', 'Luxury']
    price_sensitivities = ['Low', 'Medium', 'High']
    brand_affinities = ['Low', 'Medium', 'High']
    styles = ['Casual', 'Business', 'Luxury', 'Trendy', 'Minimalist', 'Classic']
    
    for _ in range(num_profiles):
        profile = {
            'gender': random.choice(genders),
            'age': random.choice(ages),
            'location': random.choice(locations),
            'income': random.choice(incomes),
            'price_sensitivity': random.choice(price_sensitivities),
            'brand_affinity': random.choice(brand_affinities),
            'style': random.choice(styles)
        }
        profiles.append(profile)
    
    return profiles


# ============ æµ‹è¯•ä»£ç  ============
if __name__ == "__main__":
    import yaml
    
    # åŠ è½½é…ç½®
    with open("config/config.yaml", 'r') as f:
        config = yaml.safe_load(f)
    
    # ç”Ÿæˆç”¨æˆ·ç”»åƒ
    user_profiles = generate_user_profiles(100)
    
    # å‡†å¤‡è®­ç»ƒæŸ¥è¯¢ï¼ˆä»ESCIä¸­é‡‡æ ·ï¼‰
    test_queries = [
        "summer dress",
        "men shoes",
        "wireless headphones",
        "laptop bag",
        "coffee maker"
    ]
    
    # åˆå§‹åŒ–DPOè®­ç»ƒå™¨
    trainer = OnlineDPOTrainer(
        model_path="./checkpoints/minielm",
        config=config,
        use_llm_feedback=True
    )
    
    # è®­ç»ƒ
    trainer.train(
        queries=test_queries,
        user_profiles=user_profiles,
        iterations=500
    )