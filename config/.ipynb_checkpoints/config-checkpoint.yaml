# ============ APIé…ç½® (å…³é”®ï¼) ============
api:
  provider: "openai"  # å¯é€‰: "openai" or "anthropic"
  
  # OpenAIé…ç½® (æ¨èï¼Œä¾¿å®œ)
  openai:
    api_key: "sk-YOUR_OPENAI_KEY_HERE"  # ğŸ‘ˆ åœ¨è¿™é‡Œå¡«å†™ä½ çš„OpenAI API Key
    model: "gpt-4o-mini"  # æœ€ä¾¿å®œçš„æ¨¡å‹
    base_url: "https://api.openai.com/v1"  # å¦‚æœç”¨ä¸­è½¬æœåŠ¡æ”¹è¿™é‡Œ
  
  # Anthropicé…ç½® (å¤‡é€‰)
  anthropic:
    api_key: "sk-ant-YOUR_ANTHROPIC_KEY_HERE"  # ğŸ‘ˆ æˆ–å¡«å†™Claude API Key
    model: "claude-3-haiku-20240307"

# ============ æ¨¡å‹é…ç½® ============
models:
  teacher:
    name: "google/flan-t5-large"  # 780Må‚æ•°
    load_in_8bit: false
  
  student:
    name: "google/flan-t5-base"   # 250Må‚æ•°
    load_in_8bit: false
  
  relevance_scorer:
    name: "bert-base-uncased"     # 110Må‚æ•°

# ============ æ•°æ®é…ç½® ============
data:
  esci_dataset: "amazon_esci"
  locale: "us"
  q2q_min_common_products: 5
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1

# ============ è®­ç»ƒé…ç½® ============
training:
  # SFTé…ç½®
  sft:
    num_epochs: 3
    batch_size: 8
    learning_rate: 5e-5
    gradient_accumulation_steps: 4
    fp16: true
  
  # çŸ¥è¯†è’¸é¦é…ç½®
  distillation:
    num_epochs: 3
    batch_size: 8
    learning_rate: 3e-5
    temperature: 2.0
    fp16: true
  
  # åœ¨çº¿DPOé…ç½®
  dpo:
    iterations: 500
    batch_size: 16
    learning_rate: 1e-5
    beta: 0.1
    llm_eval_frequency: 10  # æ¯10æ­¥è°ƒç”¨ä¸€æ¬¡LLM

# ============ è¯„ä¼°é…ç½® ============
evaluation:
  num_test_queries: 100
  num_rewrites_per_query: 10
  elasticsearch_top_k: 20

# ============ ç³»ç»Ÿé…ç½® ============
system:
  output_dir: "./outputs"
  checkpoint_dir: "./checkpoints"
  log_dir: "./logs"
  use_wandb: false  # æ˜¯å¦ä½¿ç”¨W&Bè®°å½•å®éªŒ
  wandb_project: "minielm"
  seed: 42